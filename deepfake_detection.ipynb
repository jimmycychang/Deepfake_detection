{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the built-in functions\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import face_recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 401\n",
      "Test samples: 400\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = './deepfake_detection/'\n",
    "TRAIN_SAMPLE_FOLDER = 'train_sample_videos'\n",
    "TEST_FOLDER = 'test_videos'\n",
    "\n",
    "print(f\"Train samples: {len(os.listdir(os.path.join(DATA_FOLDER, TRAIN_SAMPLE_FOLDER)))}\")\n",
    "print(f\"Test samples: {len(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aagfhgtpmv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vudstovrck.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapnvogymq.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>jdubbvfswz.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abarnvbtwb.mp4</th>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abofeumbvv.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>atvmxvwyns.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abqwwspghj.mp4</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>qzimuostzz.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label  split        original\n",
       "aagfhgtpmv.mp4  FAKE  train  vudstovrck.mp4\n",
       "aapnvogymq.mp4  FAKE  train  jdubbvfswz.mp4\n",
       "abarnvbtwb.mp4  REAL  train            None\n",
       "abofeumbvv.mp4  FAKE  train  atvmxvwyns.mp4\n",
       "abqwwspghj.mp4  FAKE  train  qzimuostzz.mp4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_metadata = pd.read_json('./deepfake_detection/train_sample_videos/metadata.json').T\n",
    "train_sample_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aassnaulhq.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aayfryxljh.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acazlolrpz.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adohdulfwb.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahjnxtiamx.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            video\n",
       "0  aassnaulhq.mp4\n",
       "1  aayfryxljh.mp4\n",
       "2  acazlolrpz.mp4\n",
       "3  adohdulfwb.mp4\n",
       "4  ahjnxtiamx.mp4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_videos = pd.DataFrame(list(os.listdir(os.path.join(DATA_FOLDER, TEST_FOLDER))), columns=['video'])\n",
    "test_videos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to display video\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "def play_video(video_file, subset=TRAIN_SAMPLE_FOLDER):\n",
    "\n",
    "    video_url = open(os.path.join(DATA_FOLDER, subset,video_file),'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(video_url).decode()\n",
    "    return HTML(\"\"\"<video width=500 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretreatment of videos\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "def face_loc_img (frame):\n",
    "    face_loc = face_recognition.face_locations(frame)\n",
    "    for face_location in face_loc:\n",
    "        top, right, bottom, left = face_location\n",
    "        face_image = frame[top:bottom, left:right]\n",
    "        face_image = cv2.resize(face_image, (IMG_SIZE, IMG_SIZE))\n",
    "        return face_image\n",
    "\n",
    "def load_facess(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = face_loc_img(frame)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    except Exception:\n",
    "        pass\n",
    "    else:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the original frame of video\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)\n",
    "    \n",
    "plt.imshow(load_video(\"deepfake_detection/train_sample_videos/adohikbdaz.mp4\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the facial frame of video\n",
    "\n",
    "plt.imshow(face_loc_img(load_facess(\"deepfake_detection/train_sample_videos/adohikbdaz.mp4\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show frames at intervals of 10 \n",
    "\n",
    "videoframedetect = load_video(\"deepfake_detection/train_sample_videos/adohikbdaz.mp4\")\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(15, 15))\n",
    "axs = np.array(axs)\n",
    "axs = axs.reshape(-1)\n",
    "j = 0\n",
    "for i in [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]:\n",
    "    ax = videoframedetect[i]\n",
    "    axs[j].set_title(f\"frame:{i}\")\n",
    "    axs[j].imshow(videoframedetect[i])\n",
    "    j +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 30 frames of video\n",
    "videotest = load_facess(\"deepfake_detection/train_sample_videos/adohikbdaz.mp4\")\n",
    "\n",
    "fig, axs = plt.subplots(6, 5, figsize=(15, 20))\n",
    "axs = np.array(axs)\n",
    "axs = axs.reshape(-1)\n",
    "for i in range (len(videotest)//10):\n",
    "    ax = axs[i]\n",
    "    ax.title.set_text(\"frame:\"+str(i))\n",
    "    ax.imshow(videotest[i])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show facial frames at intervals of 10 \n",
    "facialdetect = load_facess(\"deepfake_detection/train_sample_videos/adohikbdaz.mp4\")\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(15, 15))\n",
    "axs = np.array(axs)\n",
    "axs = axs.reshape(-1)\n",
    "j = 0\n",
    "for i in [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]:\n",
    "    ax = facialdetect[i]\n",
    "    axs[j].set_title(f\"frame:{i}\")\n",
    "    axs[j].imshow(facialdetect[i])\n",
    "    j +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature extraction function with Inception V3 model\n",
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretreatment of training videos\n",
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = list(df.index)\n",
    "    labels = df[\"label\"].values\n",
    "    labels = np.array(labels=='FAKE').astype(np.int)\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames = load_facess(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(380, 3) (20, 3)\n"
     ]
    }
   ],
   "source": [
    "# split training videos into training set and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Train_set, Test_set = train_test_split(train_sample_metadata,test_size=0.05,random_state=42,stratify=train_sample_metadata['label'])\n",
    "\n",
    "print(Train_set.shape, Test_set.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jimmy\\AppData\\Local\\Temp/ipykernel_17036/1857709837.py:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  labels = np.array(labels=='FAKE').astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (380, 20, 2048)\n",
      "Frame masks in train set: (380, 20)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = prepare_all_videos(Train_set, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(Test_set, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 20, 2048)]   0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, 20, 16)       99168       ['input_3[0][0]',                \n",
      "                                                                  'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 8)            624         ['gru[0][0]']                    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 8)            0           ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 8)            72          ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            9           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 99,873\n",
      "Trainable params: 99,873\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "x = keras.layers.GRU(16, return_sequences=True)(\n",
    "    frame_features_input, mask=mask_input\n",
    ")\n",
    "x = keras.layers.GRU(8)(x)\n",
    "x = keras.layers.Dropout(0.4)(x)\n",
    "x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "48/48 [==============================] - 12s 84ms/step - loss: 0.6870 - accuracy: 0.7921 - val_loss: 0.6807 - val_accuracy: 0.8000\n",
      "Epoch 2/10\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6743 - accuracy: 0.8079 - val_loss: 0.6687 - val_accuracy: 0.8000\n",
      "Epoch 3/10\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6623 - accuracy: 0.8079 - val_loss: 0.6574 - val_accuracy: 0.8000\n",
      "Epoch 4/10\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6510 - accuracy: 0.8079 - val_loss: 0.6465 - val_accuracy: 0.8000\n",
      "Epoch 5/10\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6403 - accuracy: 0.8079 - val_loss: 0.6367 - val_accuracy: 0.8000\n",
      "Epoch 6/10\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6303 - accuracy: 0.8079 - val_loss: 0.6270 - val_accuracy: 0.8000\n",
      "Epoch 7/10\n",
      "48/48 [==============================] - 1s 18ms/step - loss: 0.6206 - accuracy: 0.8079 - val_loss: 0.6183 - val_accuracy: 0.8000\n",
      "Epoch 8/10\n",
      "48/48 [==============================] - 1s 19ms/step - loss: 0.6118 - accuracy: 0.8079 - val_loss: 0.6096 - val_accuracy: 0.8000\n",
      "Epoch 9/10\n",
      "48/48 [==============================] - 1s 20ms/step - loss: 0.6034 - accuracy: 0.8079 - val_loss: 0.6018 - val_accuracy: 0.8000\n",
      "Epoch 10/10\n",
      "48/48 [==============================] - 1s 25ms/step - loss: 0.5954 - accuracy: 0.8079 - val_loss: 0.5946 - val_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint('./', save_weights_only=True, save_best_only=True)\n",
    "history = model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_data=([test_data[0], test_data[1]],test_labels),\n",
    "        callbacks=[checkpoint],\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretreatment of testing videos\n",
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    frames = load_facess(os.path.join(DATA_FOLDER, TEST_FOLDER,path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    return model.predict([frame_features, frame_mask])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoname</th>\n",
       "      <th>original_width</th>\n",
       "      <th>original_height</th>\n",
       "      <th>label</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aznyksihgl.mp4</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>xnojggkrxt.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gkwmalrvcj.mp4</td>\n",
       "      <td>129</td>\n",
       "      <td>129</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>hqqmtxvbjj.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lxnqzocgaq.mp4</td>\n",
       "      <td>223</td>\n",
       "      <td>217</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>xjzkfqddyk.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>itsbtrrelv.mp4</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>kqvepwqxfe.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ddvgrczjno.mp4</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>pluadmqqta.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        videoname  original_width  original_height label        original\n",
       "0  aznyksihgl.mp4             129              129  FAKE  xnojggkrxt.mp4\n",
       "1  gkwmalrvcj.mp4             129              129  FAKE  hqqmtxvbjj.mp4\n",
       "2  lxnqzocgaq.mp4             223              217  FAKE  xjzkfqddyk.mp4\n",
       "3  itsbtrrelv.mp4             186              186  FAKE  kqvepwqxfe.mp4\n",
       "4  ddvgrczjno.mp4             155              155  FAKE  pluadmqqta.mp4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the document for videos' information\n",
    "dff = pd.read_csv('./metadata_img.csv')\n",
    "df = pd.DataFrame(dff)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick videos randomly to test\n",
    "test_video = np.random.choice(test_videos[\"video\"].values.tolist())\n",
    "test_video_str = str(test_video)\n",
    "\n",
    "print(f\"Test video path: {test_video}\")\n",
    "df.loc[df['videoname'] == test_video_str]\n",
    "if(sequence_prediction(test_video)>=0.6):\n",
    "    c = \"FAKE\"\n",
    "    print(f'The predicted class of the video is',c)\n",
    "elif(sequence_prediction(test_video)<0.6):\n",
    "    c = \"REAL\"\n",
    "    print(f'The predicted class of the video is',c)\n",
    "\n",
    "\n",
    "try:\n",
    "    a = df.loc[df['videoname'] == test_video_str]\n",
    "    b = a.iloc[0,3]\n",
    "    print(\"The video (\"+test_video_str+\") is actually:\",b)\n",
    "    e = \"CORRECT!!\"\n",
    "    f = \"WRONG\"\n",
    "    if c == b:\n",
    "        print(\"The prediction is\",e)\n",
    "            \n",
    "    else:\n",
    "        print(\"The prediction is\",f)\n",
    "except Exception:\n",
    "     print(\"There is no existed data for the test data\")\n",
    "\n",
    "\n",
    "play_video(test_video,TEST_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy test function\n",
    "\n",
    "ass = {}\n",
    "def try_video_with_times(times):\n",
    "    for test_video in range (times):\n",
    "        test_video = np.random.choice(test_videos[\"video\"].values.tolist())\n",
    "        test_video_str = str(test_video)\n",
    "        print(f\"Test video path: {test_video}\")\n",
    "        df.loc[df['videoname'] == test_video_str]\n",
    "        if(sequence_prediction(test_video)>=0.6):\n",
    "            c = \"FAKE\"\n",
    "            print('The predicted class of the video is',c)\n",
    "        elif(sequence_prediction(test_video)<0.6):\n",
    "            c = \"REAL\"\n",
    "            print('The predicted class of the video is',c)\n",
    "\n",
    "        try: \n",
    "            a = df.loc[df['videoname'] == test_video_str]\n",
    "            b = a.iloc[0,3]\n",
    "            print(\"The video (\"+test_video_str+\") is actually:\",b)\n",
    "            e = \"CORRECT!!\"\n",
    "            f = \"WRONG\"\n",
    "            if c == b:\n",
    "                print(\"The prediction is\",e)\n",
    "                ass.update({test_video_str:\"correct\"})\n",
    "            else:\n",
    "                print(\"The prediction is\",f)\n",
    "                ass.update({test_video_str:\"incorrect\"})\n",
    "        except Exception:\n",
    "            print(\"There is no existed data for the test data\")\n",
    "\n",
    "    return ass\n",
    "\n",
    "try_video_with_times(20)\n",
    "value = list(ass.values())\n",
    "accuracy = value.count(\"correct\")/len(value)\n",
    "print(\"dict:\", ass)\n",
    "print(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function to check the value\n",
    "def check_video_value_test(videoname):\n",
    "    test_video_str = str(videoname)\n",
    "    df.loc[df['videoname'] == test_video_str]\n",
    "    print(\"Probability:\", sequence_prediction(videoname))\n",
    "    facialdetect_1 = load_facess(f\"deepfake_detection/test_videos/{videoname}\")\n",
    "    fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "    axs = np.array(axs)\n",
    "    axs = axs.reshape(-1)\n",
    "    j = 0\n",
    "    for i in [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240]:\n",
    "        ax = facialdetect_1[i]\n",
    "        axs[j].set_title(f\"frame:{i}\")\n",
    "        axs[j].imshow(facialdetect_1[i])\n",
    "        j +=1\n",
    "    plt.show()\n",
    "    if(sequence_prediction(videoname)>=0.6):\n",
    "        c = \"FAKE\"\n",
    "        print('The predicted class of the video is',c)\n",
    "    elif(sequence_prediction(videoname)<0.6):\n",
    "        c = \"REAL\"\n",
    "        print('The predicted class of the video is',c)\n",
    "    try:\n",
    "        a = df.loc[df['videoname'] == videoname]\n",
    "        b = a.iloc[0,3]\n",
    "        print(\"The video (\"+videoname+\") is actually:\",b)\n",
    "        if c == b:\n",
    "            print(\"The prediction is CORRECT!!\")\n",
    "        else:\n",
    "            print(\"The prediction is WRONG\")\n",
    "    except Exception:\n",
    "        print(\"There is no existed data for the test data\")\n",
    "    \n",
    "    return(play_video(videoname,TEST_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_video_value_test(\"aktnlyqpah.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### Videos test (10 sec videos on the Internet or input videos) ###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create functions for videos \n",
    "def play_other_video(video_file):\n",
    "    video_url = open(os.path.join(video_file),'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(video_url).decode()\n",
    "    return HTML(\"\"\"<video width=500 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)\n",
    "\n",
    "def prepare_single_video_other(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "def sequence_prediction_other(video_file):\n",
    "    frames = load_facess(os.path.join(video_file))\n",
    "    frame_features, frame_mask = prepare_single_video_other(frames)\n",
    "    return model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "# video test function\n",
    "def check_other_video_value(videoname):\n",
    "    if(sequence_prediction_other(videoname)>=0.6):\n",
    "        c = \"FAKE\"\n",
    "        print('The predicted class of the video is',c)\n",
    "    elif(sequence_prediction_other(videoname)<0.6):\n",
    "        c = \"REAL\"\n",
    "        print('The predicted class of the video is',c)\n",
    "    print(\"prediction:\",sequence_prediction_other(videoname),c)\n",
    "    # print the file name\n",
    "    print(\"The video is actually:\",videoname[22:-5])\n",
    "    a= load_facess(videoname)\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(11, 11))\n",
    "    axs = np.array(axs)\n",
    "    axs = axs.reshape(-1)\n",
    "    j = 0\n",
    "    for i in [0, 25, 50, 75, 100, 125, 150, 175, 200]:\n",
    "        ax = a[i]\n",
    "        axs[j].set_title(f\"frame:{i}\")\n",
    "        axs[j].imshow(a[i])\n",
    "        j +=1\n",
    "    video = play_other_video(videoname)\n",
    "    return(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake video test\n",
    "check_other_video_value(\"other_video_test/fake/fake1.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_other_video_value(\"other_video_test/fake/fake2.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_other_video_value(\"other_video_test/fake/fake3.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_other_video_value(\"other_video_test/fake/fake4.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real video test\n",
    "check_other_video_value(\"other_video_test/real/real1.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_other_video_value(\"other_video_test/real/real2.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
